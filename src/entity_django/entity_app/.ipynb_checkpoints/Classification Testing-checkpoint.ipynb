{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8059b7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Create a SQL connection to our SQLite database\n",
    "con = sqlite3.connect(\"C:/Users/User/OneDrive - University of Glasgow/University Year 4/Individual Project/2464980P-L4-Project/src/entity_django/db.sqlite3\")\n",
    "\n",
    "cur = con.cursor()\n",
    "\n",
    "entities={}\n",
    "for row in cur.execute('SELECT * FROM entity_app_entity;'):\n",
    "    entities[row[0]]=row[1]\n",
    "    \n",
    "instances={}\n",
    "for row in cur.execute('SELECT * FROM entity_app_instance;'):\n",
    "    documentID=row[3]\n",
    "    entityID=row[4]\n",
    "    try:\n",
    "        instances[documentID]+=entities[entityID]\n",
    "    except KeyError:\n",
    "        instances[documentID]=entities[entityID]\n",
    "\n",
    "documents={}\n",
    "for row in cur.execute('SELECT * FROM entity_app_document;'):\n",
    "    documents[row[0]]=row[2].replace(\".html.gz\",\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a1a35047",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRUTHS=\"C:/Users/User/Downloads/project_data/project_data/truths.txt\"\n",
    "with open (TRUTHS, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "truths={}\n",
    "for line in lines:\n",
    "    line=line.replace(\"\\n\",\"\").replace(\" \",\"/\").split(\"/\")[-2:]\n",
    "    truths[line[0]]=line[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "eb1ba47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(columns=['filename', 'text', 'truth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "929c0d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for docid in documents.keys():\n",
    "    try:\n",
    "        filename=documents[docid]\n",
    "        truth=int(truths[filename])\n",
    "        text=instances[docid]\n",
    "        df.loc[i] = [filename,text,truth]\n",
    "        i+=1\n",
    "    except KeyError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "96565afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "import spacy\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "11e22656",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_lg', disable=['ner'])\n",
    "nlp.remove_pipe('tagger')\n",
    "nlp.remove_pipe('parser')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a2252089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_pipeline(text):\n",
    "    tokens = []\n",
    "    doc = nlp(text)\n",
    "    for t in doc:\n",
    "        if not t.is_stop and not t.is_punct and not t.is_space:\n",
    "            tokens.append(t.lemma_.lower())\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ff962e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\spacy\\pipeline\\lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#one_hot_vectorizer = CountVectorizer(tokenizer=text_pipeline, binary=True)\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=text_pipeline)\n",
    "\n",
    "train_labels = df['truth']\n",
    "\n",
    "#train_features = one_hot_vectorizer.fit_transform(df['text'].tolist())\n",
    "tfidf_train_features = tfidf_vectorizer.fit_transform(df['text'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "41bcaae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "from sklearn.svm import SVC\n",
    "tfidf_svc=SVC(max_iter=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "35270786",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(tfidf_svc, tfidf_train_features, train_labels, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "918d3c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect(\"C:/Users/User/OneDrive - University of Glasgow/University Year 4/Individual Project/2464980P-L4-Project/src/entity_django/db.sqlite3\")\n",
    "\n",
    "cur = con.cursor()\n",
    "\n",
    "sensitivity={}\n",
    "for row in cur.execute('SELECT * FROM entity_app_instance;'):\n",
    "    documentID=row[3]\n",
    "    entityID=row[4]\n",
    "    \n",
    "    filename=documents[documentID]\n",
    "    truth=truths[filename]\n",
    "    \n",
    "    abstract=entities[entityID]\n",
    "    if abstract in list(sensitivity.keys()):\n",
    "        if truth==1 and sensitivity[abstract]==1:\n",
    "            continue\n",
    "        elif truth==1 and sensitivity[abstract]==0:\n",
    "            sensitivity[abstract]=1\n",
    "    else:\n",
    "        sensitivity[abstract]=truth\n",
    "# Be sure to close the connection\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296deb74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
